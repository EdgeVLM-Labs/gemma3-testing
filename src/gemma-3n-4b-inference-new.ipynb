{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605cabce-e404-4892-8883-f507162dc779",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    %pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
    "    %pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    %pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    %pip install --no-deps unsloth\n",
    "%pip install transformers==4.55.4\n",
    "%pip install timm\n",
    "import torch; torch._dynamo.config.recompile_limit = 64;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aedf757-aba4-4399-a8c2-87d70ca5e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3n-E4B-it\",\n",
    "    dtype = None, # None for auto detection\n",
    "    max_seq_length = 1024, # Choose any for long context!\n",
    "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de998d19-620c-409b-a54f-9bdbcf5570d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "# Helper function for inference\n",
    "def do_gemma_3n_inference(messages, max_new_tokens = 128):\n",
    "    # 1. Prepare inputs\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt = True,\n",
    "        tokenize = True,\n",
    "        return_dict = True,\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # 2. Generate\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens = max_new_tokens,\n",
    "        temperature = 0.1,\n",
    "        do_sample = True,\n",
    "        # Keep the streamer so you can still watch it work in the notebook\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    "    )\n",
    "\n",
    "    # 3. Decode only the NEW tokens (the answer)\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "    new_tokens = outputs[0][input_length:]\n",
    "    response_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2acbe0d-e590-4498-a3b2-9b69475469d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c1d9b0-e6ec-4192-85e7-aa75349d932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_frames_from_path(video_path, num_frames=8):\n",
    "    \"\"\"\n",
    "    Extract evenly spaced frames from a video file.\n",
    "\n",
    "    Args:\n",
    "        video_path: Path to video file (string or Path object)\n",
    "        num_frames: Number of frames to extract (default: 8)\n",
    "\n",
    "    Returns:\n",
    "        List of PIL Image objects\n",
    "    \"\"\"\n",
    "    video_path = str(video_path)  # Ensure it's a string\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file: {video_path}\")\n",
    "        return []\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    if total_frames == 0:\n",
    "        print(f\"Error: Video has 0 frames: {video_path}\")\n",
    "        cap.release()\n",
    "        return []\n",
    "\n",
    "    # Calculate the step size (logic from your first source)\n",
    "    step = max(1, total_frames // num_frames)\n",
    "    frames = []\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        frame_idx = min(i * step, total_frames - 1)  # Ensure we don't exceed total frames\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert BGR (OpenCV) to RGB (PIL) without rotation\n",
    "        img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        frames.append(img)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if len(frames) < num_frames:\n",
    "        print(f\"Warning: Only extracted {len(frames)}/{num_frames} frames from {video_path}\")\n",
    "\n",
    "    return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cb478a-61ef-465a-b064-ca9e2f9c9f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set up paths relative to notebook location\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\")) if \"__file__\" in dir() else os.getcwd()\n",
    "project_root = os.path.dirname(notebook_dir)  # Go up one level from src/\n",
    "video_folder = os.path.join(project_root, \"dataset\", \"videos\")\n",
    "output_csv = os.path.join(project_root, \"outputs\", \"gemma3n-4b_inference_results.csv\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Looking for videos in: {video_folder}\")\n",
    "print(f\"Will save results to: {output_csv}\")\n",
    "\n",
    "results = []\n",
    "\n",
    "# Check if video folder exists\n",
    "if not os.path.exists(video_folder):\n",
    "    print(f\"\\n⚠️  Warning: Video folder not found: {video_folder}\")\n",
    "    print(\"Please ensure videos are placed in the dataset/videos/ directory\")\n",
    "    print(\"Or update the video_folder path above to point to your videos location\")\n",
    "else:\n",
    "    # Get list of all video files recursively\n",
    "    video_files = []\n",
    "    for root, dirs, files in os.walk(video_folder):\n",
    "        for file in files:\n",
    "            if Path(file).suffix.lower() in ['.mp4', '.mov', '.avi', '.mkv']:\n",
    "                video_files.append(Path(root) / file)\n",
    "\n",
    "    print(f\"\\nFound {len(video_files)} videos. Starting processing...\\n\")\n",
    "\n",
    "    for video_path in video_files:\n",
    "        relative_path = video_path.relative_to(video_folder)\n",
    "        print(f\"Processing: {relative_path}...\")\n",
    "\n",
    "        # 1. Extract the \"Frame Thing\" (8 snapshots)\n",
    "        video_frames = extract_frames_from_path(str(video_path), num_frames=8)\n",
    "\n",
    "        if not video_frames:\n",
    "            print(f\"  ⚠️  Failed to extract frames from {relative_path}\")\n",
    "            continue\n",
    "\n",
    "        # 2. Build the message\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    *[{\"type\": \"image\", \"image\": img} for img in video_frames],\n",
    "                    {\"type\": \"text\", \"text\": \"You are an assistive navigation system for a visually impaired user. Analyze the provided video from the user's forward perspective. Identify all the immediate, high-risk obstructions. State the obstruction's location using the 12-hour clock face. Process the provided video generate a single, actionable safety alert.\"}\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        # 3. Inference\n",
    "        try:\n",
    "            response = do_gemma_3n_inference(messages, max_new_tokens=256)\n",
    "            results.append([str(relative_path), response])\n",
    "            print(f\"  ✓ Completed\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error processing {relative_path}: {e}\\n\")\n",
    "\n",
    "    # Save to CSV\n",
    "    if results:\n",
    "        with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"video_name\", \"model_output\"])\n",
    "            writer.writerows(results)\n",
    "\n",
    "        print(f\"\\n✅ Success! Saved {len(results)} results to {output_csv}\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  No results to save. Please check if videos were processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7131ee8b-6843-4aa4-9fa5-dd5c2d56a59c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
